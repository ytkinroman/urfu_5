# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [AD in GameDev]
Отчет по *Лабораторной работе №5* выполнил:
- Крюков Никита Андреевич
- РИ-230915 (AT-01)

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | ? |
| Задание 2 | * | ? |
| Задание 3 | * | ? | 

**[ Все задания выполнены ]**

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.




# Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.




# Введение

## Пример - задание №1

**Постановка задачи:** в данном задании мы создадим ML-агент и будем тренировать нейронную сеть, задача которой будет заключаться в управлении шаром. Задача шара заключается в том, чтобы оставаясь на плоскости находить кубик, смещающийся в заданном случайном диапазоне координат.

![Обучение красных шариков](img/img_3.gif)

Создал и настроил сцену согласна данным из методичной работы и видео с объяснением преподавателя. От себя добавил несколько более приятных материалов для сцены, простенький UI со шрифтами, по ходу выполнения задания старался соблюдать структуру проекта, а так же сам по себе проект сделал не обычный Unity 3D, а Unity URP проект, чтобы глаза не резало (более комфортно для меня). Начал обучение согласно заданию и успешно его закончил (результаты эпох обучения на фотографии ниже).

![Результаты обучения](img/img_1.jpg)

После завершения обучения протестировал, всё успешно работает. Бывает иногда ошибки обучения из-за недостаточного количества эпох... Например если зелёный кубик появился где-то далеко от красного шарика ближе к краю платформы, то шарик прикатившись к кубику не всегда на перед понимает где может появится кубик заново. Если кубик появятся где-то за спиной (грубо говоря) шарика, то шарик не успевает затормозить и вылетает с платформы. Это то, что я мог наблюдать в своём эксперименте.

Я попробовал повторить обучения, случаи когда шарик вылетает за платформы уменьшились, но полностью случаи не исключены, возможно дело в очень ограниченном пространстве и сильном разбросе появления зелёного кубика. Как мне кажется падения в этой задаче можно свести к минимуму достаточным обучением, но это не исключит такие ситуации (смотрел видео где какой-то мужчина заставлял шар удержатся на вращающийся платформе, там были моменты в который вообще не как не избежать падения из-за физики движения сферы).

![Результаты повторного обучения](img/img_4.jpg)

Загрузил Unity проект с решением в репозиторий, в директорию MlAgentProject. Посмотреть можно [тут](https://github.com/ytkinroman/urfu_5/blob/main/MlAgentProject). Результат выполнения ниже:

![Результаты работы проекта](img/img_2.gif)




## Пример - задание №2
Согласно методическим указаниям скачал проект, настроил и запустил обучение. После завершения обучения протестировал.

Процесс обучения:

![Процесс обучения](img/img_5.gif)

Результаты обучения:

![Тестирование работы](img/img_6.jpg)

Загрузил Unity проект с решением в репозиторий, в директорию ML-Agent_EconomicModel. Посмотреть можно [тут](https://github.com/ytkinroman/urfu_5/blob/main/ML-Agent_EconomicModel). Тестирование обучения ниже:

![Результаты работы проекта](img/img_7.gif)

График для оценки результатов обучения:

![График результатов обучения](img/img_8.jpg)

Как можем рассмотреть график я немного перестарался с обучением. Агент сначала обучался очень даже хорошо, но затем произошло снижение эффективности из-за переобучения (резкие падение Cumulative Reward может свидетельствовать о проблемах в обучении).




# Задания к работе

**Примечание:** буду отвечать на задания к работе на примере первого агента т.к я самостоятельно создал его по записи с лекции и лучше разобрался в его работе.


## Задание 1
*Найдите внутри C# скрипта "коэффициент корреляции" и сделать выводы о том, как он влияет на обучение модели.*

Что такое корреляция? В рамках задачи, корреляция можно определить как меру, которая показывает, насколько изменения в одном параметре связаны с изменениями в другом параметре. Проще говоря, корреляция в этой задаче означает, насколько сильно и как изменения в одном параметре соответствуют изменениям в другом параметре. Это помогает понять, насколько эффективно агент реагирует на изменения в окружающей среде и как он приближается к цели.

В предоставленном скрипте коэффициент корреляции не используется напрямую. Однако, можно рассмотреть, как различные параметры и наблюдения могут коррелировать друг с другом и как это может влиять на обучение агента. Такими параметрами могут являться сила реакции *public float forceMultiplier = 10;* или максимальное расстояние, на которое агент должен приблизится к цели *1.42f*.

Я попробовал провести эксперимент и максимальное расстояние, на которое агент должен приблизится к цели до значения *2.1f*. 

![Обучение с измененным параметром](img/img_9.jpg)

![Консоль обучения с измененными параметрами](img/img_10.jpg)

![Графики обучения агента](img/img_11.jpg)

Среднее вознаграждение (Mean Reward) постепенно увеличивается с течением времени. Начальное значение около 0.598 на шаге 10000 увеличивается до 1.000 на шаге 120000 и далее. Это указывает на то, что агент становится все более эффективным в достижении своей цели. После шага 110000 среднее вознаграждение стабилизируется на уровне 1.000, что свидетельствует о том, что агент достиг максимальной эффективности в выполнении своей задачи. Стандартное отклонение вознаграждения (Std of Reward) также уменьшается с течением времени. Начальное значение около 0.490 на шаге 10000 снижается до 0.000 на шаге 120000 и далее. Это указывает на то, что вознаграждения становятся более стабильными и предсказуемыми, что является признаком успешного обучения.

Исходя из эксперимента, можно сказать, что коэффициент корреляции в данном случае будет влиять на то, насколько сильно агент будет стремиться к своей цели и выискивать идеальную точку для попадания, стараясь минимизировать среднее отклонение от этой точки.


## Задание 2
*Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.*

```
behaviors:
  RollerBall:
    trainer_type: ppo        # Тип тренера, используемый для обучения агента
    hyperparameters:
      batch_size: 10         # Размер пакета данных, используемого для обновления модели
      buffer_size: 100       # Размер буфера опыта, используемого для хранения данных о взаимодействиях агента с окружающей средой
      learning_rate: 3.0e-4  # Скорость обучения, определяющая шаг градиентного спуска
      beta: 5.0e-4           # Коэффициент регуляризации для энтропии, используемый для поощрения исследования
      epsilon: 0.2       # Коэффициент, используемый для ограничения изменений политики
      lambd: 0.99        # Коэффициент, используемый для вычисления обобщенного преимущества 
      num_epoch: 3       # Количество эпох обучения на одном пакете данных
      learning_rate_schedule: linear  # Схема изменения скорости обучения (линейное уменьшение)
    network_settings:
      normalize: false   # Флаг, указывающий, нужно ли нормализовать входные данные
      hidden_units: 128  # Количество нейронов в скрытых слоях нейронной сети
      num_layers: 2      # Количество скрытых слоев в нейронной сети
    reward_signals:
      extrinsic:
        gamma: 0.99      # Коэффициент дисконтирования для внешнего сигнала вознаграждения
        strength: 1.0    # Сила внешнего сигнала вознаграждения
    max_steps: 500000    # Максимальное количество шагов обучения
    time_horizon: 64     # Длина временного горизонта для одного эпизода
    summary_freq: 10000  # Частота вывода сводной информации о процессе обучения
```

Параметр buffer_size определяет размер буфера опыта, который используется для хранения данных о взаимодействиях агента с окружающей средой. Этот буфер содержит информацию о состояниях, действиях, вознаграждениях и следующих состояниях. Я попробовал изменить значение batch_size, увеличив его с 10 до 50. Основное влияние этого изменения заключалось в сокращении времени обучения, которое уменьшилось в несколько раз.

Параметр num_epoch определяет количество эпох обучения, которые будут проведены на одном пакете данных. Большее количество эпох может улучшить качество обучения, так как модель будет иметь больше возможностей для обновления своих весов на основе одного и того же пакета данных, но в данном случае видимых изменений нет, так как изначально модель не требует большого количества эпох для обучения.

Параметр learning_rate определяет шаг градиентного спуска, который используется для обновления весов нейронной сети. Этот параметр контролирует, насколько сильно модель изменяет свои веса на каждом шаге обучения. Более высокая скорость обучения может ускорить процесс обучения, но также может привести к нестабильности и колебаниям в процессе обучения. Это может привести к тому, что модель будет "перескакивать" через оптимальные точки. На практике особо ничего не изменилось, возможно стоило больше изменить параметры.


## Задание 3
*Приведите примеры, для каких игровых задачи и ситуаций могут использоваться первый и второй примеры с ML-Agent'ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?*

Использование ML-Агентов может быть полезным в различных игровых задачах и ситуациях, где традиционные программные решения могут быть неэффективными. 

Первый агент, предназначенный для поиска постоянно перемещающейся цели, может быть использован в ситуациях, где у нас есть постоянно передвигающийся враждебные NPC. В таком случае NPC будут следовать за целью, координаты которой постоянно меняются, особенно если это происходит в 3D пространстве, где есть дополнительные препятствия, которые NPC не может преодолеть (они физические для него). В таком случае у противника будет цель не только найти игрока, но и сделать это самым оптимальным способом. ML-агент может адаптироваться к изменениям в окружающей среде и разрабатывать сложные стратегии, которые трудно запрограммировать вручную. Агент может учиться на основе своих ошибок и успехов, улучшая свою стратегию со временем. Как мне кажется такой вариант использование ML-Агента лучше адаптировать для постоянно изменяющегося уровня с динамичной и сложной структурой препятствий, иначе мне кажется наличие ML-Агента избыточной.

Второй агент, представляющий собой NPC, занимающегося добычей золота в руднике, может быть использован в играх, где NPC-персонажи должны выполнять некоторую монотонную работу. Например, это применимо для игр в жанре пошаговой стратегии, в частности если там есть элементы градостроительства и ресурсного менеджмента. В таком случае, если игрок переопределяет цель, то персонажу легко перестроиться на новый путь следования, особенно с учетом постоянно меняющегося окружения. NPC должен находить наиболее эффективные пути для добычи золота, избегать опасных зон или врагов, а также управлять своими ресурсами, такими как инструменты и здоровье. ML-агент может находить оптимальные маршруты и стратегии для добычи золота, учиться избегать опасностей и эффективно управлять своими ресурсами.

Как мне кажется, оптимально использовать ML-агентов в играх, где окружение представляет сложную структуру или постоянно меняется состояние окружения. Уже существует множество механик, которые заменяют ML-Агента, поэтому я не вижу причин для использования ML-агентов, возможно их стоит использовать, когда ты обладаешь вычислительными мощностями и тебе нужно быстро реализовать какую-то простою механику используя ML-Агента.



# Выводы
В процессе работы я ознакомился с различными программными средствами, используемыми для создания системы машинного обучения в Unity. Я детально разобрал два примера ML-Агентов: первый агент был предназначен для поиска постоянно перемещающейся цели, а второй агент представлял собой NPC, занимающегося добычей золота в руднике. В рамках первого задания я создал несколько вариаций с различными конфигурациями обучения и протестировал изменения.

Буду ждать комментариев по поводу моего отчёта, хорошего Вам дня !

![Картинка из игры Deadlock](img/img_end.jpg)
